# llm_speculate_decoding
evaluate Speculative Decoding that promising 2-3X speedups of LLM inference by running two models in parallel.
